# Pandasç»Ÿä¸€æ•°æ®æ¨¡å‹ä¸è´¨é‡ç›‘æ§ä½“ç³»å®ç°æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨**Pandas**æ„å»ºç»Ÿä¸€æ•°æ®æ¨¡å‹å’Œå®Œæ•´çš„è´¨é‡ç›‘æ§ä½“ç³»ï¼ŒåŸºäºPrometheus Global Guardiané¡¹ç›®ä¸­USGSã€NASA EONETã€GDACSä¸‰å¤§æ•°æ®æºçš„å®é™…æ•´åˆç»éªŒã€‚

---

## ä¸€ã€ç»Ÿä¸€æ•°æ®æ¨¡å‹è®¾è®¡

### 1.1 é—®é¢˜èƒŒæ™¯

**å¤šæºå¼‚æ„æ•°æ®æŒ‘æˆ˜**ï¼š
- **USGSåœ°éœ‡æ•°æ®**ï¼šGeoJSONæ ¼å¼ï¼ŒåŒ…å«éœ‡çº§(magnitude)ã€æ·±åº¦(depth)ã€ä½ç½®åæ ‡
- **NASA EONET**ï¼šJSONæ ¼å¼ï¼ŒåŒ…å«äº‹ä»¶åˆ†ç±»(category)ã€æ—¶é—´è½¨è¿¹(geometry)ã€æŒç»­æ—¶é—´
- **GDACSé¢„è­¦**ï¼šRSS/XMLæ ¼å¼ï¼ŒåŒ…å«ä¸¥é‡æ€§çº§åˆ«(alertlevel)ã€å—å½±å“äººå£(population)

æ¯ä¸ªæ•°æ®æºçš„å­—æ®µå‘½åã€æ•°æ®ç±»å‹ã€æ—¶é—´æ ¼å¼éƒ½ä¸ä¸€è‡´ï¼Œéœ€è¦ç»Ÿä¸€å¤„ç†ã€‚

---

### 1.2 Pandasç»Ÿä¸€æ•°æ®æ¨¡å‹å®ç°

#### **æ­¥éª¤1ï¼šå®šä¹‰æ ‡å‡†åŒ–DataFrame Schema**

```python
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Dict, Optional

class UnifiedHazardModel:
    """ç»Ÿä¸€çš„ç¾å®³æ•°æ®æ¨¡å‹"""
    
    # å®šä¹‰æ ‡å‡†åŒ–çš„DataFrameåˆ—åå’Œæ•°æ®ç±»å‹
    SCHEMA = {
        'id': 'string',                    # å”¯ä¸€æ ‡è¯†ç¬¦
        'title': 'string',                 # äº‹ä»¶æ ‡é¢˜
        'type': 'category',                # ç¾å®³ç±»å‹ï¼ˆåˆ†ç±»æ•°æ®ï¼‰
        'severity': 'category',            # ä¸¥é‡æ€§ç­‰çº§ï¼ˆWARNING/WATCH/ADVISORYï¼‰
        'description': 'string',           # è¯¦ç»†æè¿°
        'latitude': 'float64',             # çº¬åº¦
        'longitude': 'float64',            # ç»åº¦
        'magnitude': 'float64',            # éœ‡çº§/å¼ºåº¦ï¼ˆå¯é€‰ï¼‰
        'timestamp': 'datetime64[ns]',    # æ—¶é—´æˆ³ï¼ˆæ ‡å‡†åŒ–ä¸ºdatetimeï¼‰
        'source': 'category',              # æ•°æ®æ¥æºï¼ˆUSGS/NASA/GDACSï¼‰
        'population_exposed': 'Int64',     # å—å½±å“äººå£ï¼ˆå¯ç©ºæ•´æ•°ï¼‰
        'data_quality_score': 'float64'   # æ•°æ®è´¨é‡åˆ†æ•°
    }
    
    @classmethod
    def create_empty_dataframe(cls) -> pd.DataFrame:
        """åˆ›å»ºç¬¦åˆSchemaçš„ç©ºDataFrame"""
        df = pd.DataFrame(columns=cls.SCHEMA.keys())
        return df.astype(cls.SCHEMA)
```

#### **æ­¥éª¤2ï¼šå„æ•°æ®æºçš„Pandasè½¬æ¢å™¨**

**USGSåœ°éœ‡æ•°æ®è½¬æ¢**ï¼š
```python
def transform_usgs_to_unified(usgs_data: Dict) -> pd.DataFrame:
    """
    å°†USGS GeoJSONæ ¼å¼è½¬æ¢ä¸ºç»Ÿä¸€Pandas DataFrame
    
    è¾“å…¥ç¤ºä¾‹ï¼š
    {
        "features": [
            {
                "id": "us6000abcd",
                "properties": {
                    "mag": 5.3,
                    "place": "10 km NE of Tokyo",
                    "time": 1638316800000,
                    "title": "M 5.3 - 10 km NE of Tokyo"
                },
                "geometry": {
                    "coordinates": [139.7, 35.7, 10]
                }
            }
        ]
    }
    """
    records = []
    
    for feature in usgs_data.get('features', []):
        props = feature['properties']
        coords = feature['geometry']['coordinates']
        
        record = {
            'id': feature['id'],
            'title': props.get('title', ''),
            'type': 'EARTHQUAKE',  # USGSåªæä¾›åœ°éœ‡æ•°æ®
            'severity': _map_magnitude_to_severity(props.get('mag')),
            'description': props.get('place', ''),
            'latitude': coords[1],
            'longitude': coords[0],
            'magnitude': props.get('mag'),
            'timestamp': pd.to_datetime(props['time'], unit='ms'),  # æ¯«ç§’è½¬datetime
            'source': 'USGS',
            'population_exposed': None,
            'data_quality_score': None
        }
        records.append(record)
    
    df = pd.DataFrame(records)
    return df.astype(UnifiedHazardModel.SCHEMA)

def _map_magnitude_to_severity(magnitude: Optional[float]) -> str:
    """éœ‡çº§åˆ°ä¸¥é‡æ€§ç­‰çº§çš„æ˜ å°„"""
    if magnitude is None:
        return 'ADVISORY'
    if magnitude >= 7.0:
        return 'WARNING'
    elif magnitude >= 5.0:
        return 'WATCH'
    else:
        return 'ADVISORY'
```

**NASA EONETæ•°æ®è½¬æ¢**ï¼š
```python
def transform_nasa_to_unified(nasa_data: Dict) -> pd.DataFrame:
    """
    å°†NASA EONET JSONæ ¼å¼è½¬æ¢ä¸ºç»Ÿä¸€Pandas DataFrame
    
    è¾“å…¥ç¤ºä¾‹ï¼š
    {
        "events": [
            {
                "id": "EONET_123",
                "title": "Wildfire - California",
                "categories": [{"title": "Wildfires"}],
                "geometry": [
                    {
                        "date": "2024-12-01T12:00:00Z",
                        "type": "Point",
                        "coordinates": [-120.5, 37.8]
                    }
                ]
            }
        ]
    }
    """
    records = []
    
    for event in nasa_data.get('events', []):
        # æå–æœ€æ–°çš„åœ°ç†ä½ç½®
        latest_geo = event['geometry'][-1] if event.get('geometry') else {}
        coords = latest_geo.get('coordinates', [0, 0])
        
        record = {
            'id': event['id'],
            'title': event.get('title', ''),
            'type': _map_nasa_category(event['categories'][0]['title']),
            'severity': 'WATCH',  # NASAé»˜è®¤ä¸ºWATCHçº§åˆ«
            'description': event.get('description', ''),
            'latitude': coords[1] if len(coords) > 1 else None,
            'longitude': coords[0] if len(coords) > 0 else None,
            'magnitude': None,
            'timestamp': pd.to_datetime(latest_geo.get('date')),  # ISO 8601æ ¼å¼
            'source': 'NASA',
            'population_exposed': None,
            'data_quality_score': None
        }
        records.append(record)
    
    df = pd.DataFrame(records)
    return df.astype(UnifiedHazardModel.SCHEMA)

def _map_nasa_category(category: str) -> str:
    """NASAåˆ†ç±»åˆ°æ ‡å‡†ç±»å‹çš„æ˜ å°„"""
    category_lower = category.lower()
    mapping = {
        'wildfires': 'WILDFIRE',
        'volcanoes': 'VOLCANO',
        'severe storms': 'STORM',
        'floods': 'FLOOD',
        'drought': 'DROUGHT',
        'earthquakes': 'EARTHQUAKE'
    }
    return mapping.get(category_lower, 'OTHER')
```

**GDACSé¢„è­¦æ•°æ®è½¬æ¢**ï¼š
```python
def transform_gdacs_to_unified(gdacs_data: Dict) -> pd.DataFrame:
    """
    å°†GDACSé¢„è­¦æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€Pandas DataFrame
    
    è¾“å…¥ç¤ºä¾‹ï¼š
    {
        "items": [
            {
                "id": "GDACS_456",
                "title": "Cyclone Alert - Bay of Bengal",
                "alertlevel": "Orange",
                "population": {"value": 1500000},
                "pubDate": "Mon, 01 Dec 2024 14:30:00 GMT",
                "point": {"lat": 18.5, "lon": 88.3}
            }
        ]
    }
    """
    records = []
    
    for item in gdacs_data.get('items', []):
        record = {
            'id': item['id'],
            'title': item.get('title', ''),
            'type': _detect_type_from_title(item['title']),
            'severity': _map_gdacs_alertlevel(item.get('alertlevel')),
            'description': item.get('description', ''),
            'latitude': item['point']['lat'],
            'longitude': item['point']['lon'],
            'magnitude': None,
            'timestamp': pd.to_datetime(item['pubDate']),  # RFC 2822æ ¼å¼
            'source': 'GDACS',
            'population_exposed': item['population']['value'],
            'data_quality_score': None
        }
        records.append(record)
    
    df = pd.DataFrame(records)
    return df.astype(UnifiedHazardModel.SCHEMA)

def _map_gdacs_alertlevel(alertlevel: str) -> str:
    """GDACSé¢„è­¦çº§åˆ«åˆ°æ ‡å‡†ä¸¥é‡æ€§çš„æ˜ å°„"""
    mapping = {
        'Red': 'WARNING',
        'Orange': 'WATCH',
        'Green': 'ADVISORY'
    }
    return mapping.get(alertlevel, 'ADVISORY')

def _detect_type_from_title(title: str) -> str:
    """ä»æ ‡é¢˜æ™ºèƒ½è¯†åˆ«ç¾å®³ç±»å‹"""
    title_lower = title.lower()
    if 'cyclone' in title_lower or 'hurricane' in title_lower or 'typhoon' in title_lower:
        return 'TROPICAL_CYCLONE'
    elif 'flood' in title_lower:
        return 'FLOOD'
    elif 'earthquake' in title_lower:
        return 'EARTHQUAKE'
    elif 'tsunami' in title_lower:
        return 'TSUNAMI'
    elif 'volcano' in title_lower:
        return 'VOLCANO'
    return 'OTHER'
```

#### **æ­¥éª¤3ï¼šåˆå¹¶å¤šæºæ•°æ®**

```python
def merge_multi_source_data(
    usgs_df: pd.DataFrame,
    nasa_df: pd.DataFrame,
    gdacs_df: pd.DataFrame
) -> pd.DataFrame:
    """
    åˆå¹¶ä¸‰ä¸ªæ•°æ®æºä¸ºç»Ÿä¸€DataFrame
    ä½¿ç”¨Pandas concatå®ç°é«˜æ•ˆåˆå¹¶
    """
    # ä½¿ç”¨pd.concatçºµå‘å †å ï¼ˆaxis=0ï¼‰
    unified_df = pd.concat(
        [usgs_df, nasa_df, gdacs_df],
        axis=0,
        ignore_index=True,  # é‡ç½®ç´¢å¼•
        sort=False
    )
    
    # æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    unified_df = unified_df.sort_values('timestamp', ascending=False)
    
    # å»é™¤å®Œå…¨é‡å¤çš„è®°å½•
    unified_df = unified_df.drop_duplicates(subset=['id'], keep='first')
    
    # é‡ç½®ç´¢å¼•
    unified_df = unified_df.reset_index(drop=True)
    
    return unified_df
```

---

## äºŒã€Pandasè´¨é‡ç›‘æ§ä½“ç³»

### 2.1 äº”ç»´æ•°æ®è´¨é‡è¯„ä¼°

```python
class DataQualityMonitor:
    """åŸºäºPandasçš„æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.quality_report = {}
    
    def assess_data_quality(self) -> Dict:
        """
        æ‰§è¡Œå…¨é¢çš„æ•°æ®è´¨é‡è¯„ä¼°
        è¿”å›åŒ…å«5ä¸ªç»´åº¦çš„è´¨é‡æŠ¥å‘Š
        """
        self.quality_report = {
            'completeness': self._check_completeness(),
            'accuracy': self._check_accuracy(),
            'consistency': self._check_consistency(),
            'timeliness': self._check_timeliness(),
            'validity': self._check_validity(),
            'overall_score': 0.0
        }
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            'completeness': 0.25,
            'accuracy': 0.25,
            'consistency': 0.20,
            'timeliness': 0.15,
            'validity': 0.15
        }
        
        self.quality_report['overall_score'] = sum(
            self.quality_report[dim]['score'] * weights[dim]
            for dim in weights.keys()
        )
        
        return self.quality_report
```

#### **ç»´åº¦1ï¼šå®Œæ•´æ€§æ£€æŸ¥ (Completeness)**

```python
def _check_completeness(self) -> Dict:
    """
    æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼šç¼ºå¤±å€¼åˆ†æ
    ä½¿ç”¨Pandasçš„isnull()å’Œvalue_counts()
    """
    total_records = len(self.df)
    
    # å¿…å¡«å­—æ®µ
    required_fields = ['id', 'title', 'type', 'severity', 'timestamp', 'latitude', 'longitude']
    
    # è®¡ç®—æ¯ä¸ªå­—æ®µçš„ç¼ºå¤±ç‡
    missing_stats = {}
    for field in required_fields:
        missing_count = self.df[field].isnull().sum()
        missing_rate = (missing_count / total_records) * 100
        missing_stats[field] = {
            'missing_count': int(missing_count),
            'missing_rate': round(missing_rate, 2)
        }
    
    # å®Œå…¨ç¼ºå¤±å¿…å¡«å­—æ®µçš„è®°å½•æ•°
    incomplete_records = self.df[required_fields].isnull().any(axis=1).sum()
    
    # è®¡ç®—å®Œæ•´æ€§åˆ†æ•° (100 - å¹³å‡ç¼ºå¤±ç‡)
    avg_missing_rate = np.mean([stat['missing_rate'] for stat in missing_stats.values()])
    completeness_score = max(0, 100 - avg_missing_rate)
    
    return {
        'score': completeness_score,
        'missing_stats': missing_stats,
        'incomplete_records': int(incomplete_records),
        'completeness_rate': round((total_records - incomplete_records) / total_records * 100, 2)
    }
```

#### **ç»´åº¦2ï¼šå‡†ç¡®æ€§æ£€æŸ¥ (Accuracy)**

```python
def _check_accuracy(self) -> Dict:
    """
    æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§ï¼šåæ ‡èŒƒå›´ã€éœ‡çº§åˆç†æ€§
    ä½¿ç”¨Pandasçš„æ¡ä»¶è¿‡æ»¤å’Œé€»è¾‘è¿ç®—
    """
    total = len(self.df)
    issues = []
    
    # æ£€æŸ¥1ï¼šç»çº¬åº¦èŒƒå›´æœ‰æ•ˆæ€§
    invalid_coords = self.df[
        (self.df['latitude'] < -90) | (self.df['latitude'] > 90) |
        (self.df['longitude'] < -180) | (self.df['longitude'] > 180)
    ]
    coord_accuracy = (total - len(invalid_coords)) / total * 100
    
    if len(invalid_coords) > 0:
        issues.append(f"{len(invalid_coords)} records with invalid coordinates")
    
    # æ£€æŸ¥2ï¼šéœ‡çº§åˆç†æ€§ï¼ˆåœ°éœ‡æ•°æ®ï¼‰
    earthquake_df = self.df[self.df['type'] == 'EARTHQUAKE']
    if len(earthquake_df) > 0:
        invalid_magnitude = earthquake_df[
            (earthquake_df['magnitude'] < 0) | (earthquake_df['magnitude'] > 10)
        ]
        if len(invalid_magnitude) > 0:
            issues.append(f"{len(invalid_magnitude)} earthquakes with unrealistic magnitude")
    
    # æ£€æŸ¥3ï¼šæ—¶é—´æˆ³æ ¼å¼éªŒè¯
    invalid_timestamps = self.df[self.df['timestamp'].isnull()].shape[0]
    timestamp_accuracy = (total - invalid_timestamps) / total * 100
    
    # ç»¼åˆå‡†ç¡®æ€§åˆ†æ•°
    accuracy_score = (coord_accuracy + timestamp_accuracy) / 2
    
    return {
        'score': accuracy_score,
        'coord_accuracy': round(coord_accuracy, 2),
        'timestamp_accuracy': round(timestamp_accuracy, 2),
        'issues': issues,
        'invalid_records': len(invalid_coords) + len(invalid_magnitude) + invalid_timestamps
    }
```

#### **ç»´åº¦3ï¼šä¸€è‡´æ€§æ£€æŸ¥ (Consistency)**

```python
def _check_consistency(self) -> Dict:
    """
    æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼šç±»å‹æšä¸¾ã€ä¸¥é‡æ€§çº§åˆ«æ ‡å‡†åŒ–
    ä½¿ç”¨Pandasçš„unique()å’Œisin()
    """
    # å®šä¹‰æ ‡å‡†æšä¸¾å€¼
    VALID_TYPES = [
        'EARTHQUAKE', 'VOLCANO', 'STORM', 'FLOOD', 'WILDFIRE',
        'DROUGHT', 'TROPICAL_CYCLONE', 'TSUNAMI', 'LANDSLIDE', 'OTHER'
    ]
    VALID_SEVERITIES = ['WARNING', 'WATCH', 'ADVISORY']
    VALID_SOURCES = ['USGS', 'NASA', 'GDACS']
    
    total = len(self.df)
    inconsistencies = []
    
    # æ£€æŸ¥ç±»å‹ä¸€è‡´æ€§
    invalid_types = self.df[~self.df['type'].isin(VALID_TYPES)]
    type_consistency = (total - len(invalid_types)) / total * 100
    if len(invalid_types) > 0:
        inconsistencies.append(
            f"{len(invalid_types)} records with invalid type: "
            f"{invalid_types['type'].unique().tolist()}"
        )
    
    # æ£€æŸ¥ä¸¥é‡æ€§ä¸€è‡´æ€§
    invalid_severity = self.df[~self.df['severity'].isin(VALID_SEVERITIES)]
    severity_consistency = (total - len(invalid_severity)) / total * 100
    if len(invalid_severity) > 0:
        inconsistencies.append(
            f"{len(invalid_severity)} records with invalid severity"
        )
    
    # æ£€æŸ¥æ•°æ®æºä¸€è‡´æ€§
    invalid_source = self.df[~self.df['source'].isin(VALID_SOURCES)]
    source_consistency = (total - len(invalid_source)) / total * 100
    
    # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
    consistency_score = (type_consistency + severity_consistency + source_consistency) / 3
    
    return {
        'score': consistency_score,
        'type_consistency': round(type_consistency, 2),
        'severity_consistency': round(severity_consistency, 2),
        'source_consistency': round(source_consistency, 2),
        'inconsistencies': inconsistencies
    }
```

#### **ç»´åº¦4ï¼šåŠæ—¶æ€§æ£€æŸ¥ (Timeliness)**

```python
def _check_timeliness(self) -> Dict:
    """
    æ£€æŸ¥æ•°æ®åŠæ—¶æ€§ï¼š7å¤©å†…çš„æ•°æ®å æ¯”
    ä½¿ç”¨Pandasçš„datetimeè®¡ç®—
    """
    now = pd.Timestamp.now(tz='UTC')
    seven_days_ago = now - pd.Timedelta(days=7)
    
    # ç¡®ä¿timestampåˆ—ä¸ºdatetimeç±»å‹
    self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], utc=True)
    
    # ç»Ÿè®¡7å¤©å†…çš„æ•°æ®
    recent_data = self.df[self.df['timestamp'] >= seven_days_ago]
    timeliness_rate = len(recent_data) / len(self.df) * 100
    
    # è®¡ç®—æ•°æ®å¹³å‡å¹´é¾„
    self.df['data_age_hours'] = (now - self.df['timestamp']).dt.total_seconds() / 3600
    avg_age_hours = self.df['data_age_hours'].mean()
    
    # åŠæ—¶æ€§åˆ†æ•°ï¼š7å¤©å†…æ•°æ®æ¯”ä¾‹
    timeliness_score = timeliness_rate
    
    return {
        'score': timeliness_score,
        'recent_data_count': int(len(recent_data)),
        'recent_data_rate': round(timeliness_rate, 2),
        'avg_age_hours': round(avg_age_hours, 2),
        'oldest_record': str(self.df['timestamp'].min()),
        'newest_record': str(self.df['timestamp'].max())
    }
```

#### **ç»´åº¦5ï¼šæœ‰æ•ˆæ€§æ£€æŸ¥ (Validity)**

```python
def _check_validity(self) -> Dict:
    """
    æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§ï¼šä¸šåŠ¡è§„åˆ™éªŒè¯
    ä½¿ç”¨Pandasçš„é«˜çº§è¿‡æ»¤å’Œèšåˆ
    """
    total = len(self.df)
    validation_issues = []
    
    # è§„åˆ™1ï¼šåœ°éœ‡å¿…é¡»æœ‰éœ‡çº§
    earthquakes_without_mag = self.df[
        (self.df['type'] == 'EARTHQUAKE') & 
        (self.df['magnitude'].isnull())
    ]
    if len(earthquakes_without_mag) > 0:
        validation_issues.append(
            f"{len(earthquakes_without_mag)} earthquakes missing magnitude"
        )
    
    # è§„åˆ™2ï¼šWARNINGçº§åˆ«äº‹ä»¶åº”è¯¥æœ‰å—å½±å“äººå£æ•°æ®ï¼ˆGDACSï¼‰
    warnings_without_population = self.df[
        (self.df['severity'] == 'WARNING') & 
        (self.df['source'] == 'GDACS') & 
        (self.df['population_exposed'].isnull())
    ]
    if len(warnings_without_population) > 0:
        validation_issues.append(
            f"{len(warnings_without_population)} WARNING events missing population data"
        )
    
    # è§„åˆ™3ï¼šæ ‡é¢˜å’Œæè¿°ä¸åº”ä¸ºç©º
    empty_title = self.df[self.df['title'].str.strip() == ''].shape[0]
    if empty_title > 0:
        validation_issues.append(f"{empty_title} records with empty title")
    
    # è®¡ç®—æœ‰æ•ˆæ€§åˆ†æ•°
    total_validation_issues = (
        len(earthquakes_without_mag) + 
        len(warnings_without_population) + 
        empty_title
    )
    validity_score = max(0, (total - total_validation_issues) / total * 100)
    
    return {
        'score': validity_score,
        'validation_issues': validation_issues,
        'invalid_records': total_validation_issues,
        'validity_rate': round(validity_score, 2)
    }
```

### 2.2 è‡ªåŠ¨åŒ–è´¨é‡ä¿®å¤

```python
def auto_fix_quality_issues(df: pd.DataFrame) -> pd.DataFrame:
    """
    è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„è´¨é‡é—®é¢˜
    è¿”å›ä¿®å¤åçš„DataFrame
    """
    df_fixed = df.copy()
    
    # ä¿®å¤1ï¼šæ ‡å‡†åŒ–å­—ç¬¦ä¸²ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
    string_columns = ['title', 'description']
    for col in string_columns:
        df_fixed[col] = df_fixed[col].str.strip()
    
    # ä¿®å¤2ï¼šå¡«å……é»˜è®¤å€¼
    df_fixed['description'] = df_fixed['description'].fillna('No description available')
    
    # ä¿®å¤3ï¼šç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
    df_fixed.loc[df_fixed['latitude'] < -90, 'latitude'] = -90
    df_fixed.loc[df_fixed['latitude'] > 90, 'latitude'] = 90
    df_fixed.loc[df_fixed['longitude'] < -180, 'longitude'] = -180
    df_fixed.loc[df_fixed['longitude'] > 180, 'longitude'] = 180
    
    # ä¿®å¤4ï¼šåˆ é™¤å®Œå…¨æ— æ•ˆçš„è®°å½•ï¼ˆç¼ºå°‘å¿…å¡«å­—æ®µï¼‰
    required_fields = ['id', 'title', 'type', 'severity', 'timestamp']
    df_fixed = df_fixed.dropna(subset=required_fields)
    
    return df_fixed
```

### 2.3 è´¨é‡åˆ†æ•°æŒä¹…åŒ–

```python
def add_quality_score_to_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¸ºæ¯æ¡è®°å½•æ·»åŠ æ•°æ®è´¨é‡åˆ†æ•°
    åŸºäºè®°å½•çº§åˆ«çš„è´¨é‡æ£€æŸ¥
    """
    df_scored = df.copy()
    
    def calculate_record_quality(row) -> float:
        """è®¡ç®—å•æ¡è®°å½•çš„è´¨é‡åˆ†æ•°"""
        score = 100.0
        
        # ç¼ºå¤±å¿…å¡«å­—æ®µæ‰£åˆ†
        required_fields = ['id', 'title', 'type', 'severity', 'timestamp', 'latitude', 'longitude']
        for field in required_fields:
            if pd.isnull(row[field]):
                score -= 15
        
        # åæ ‡æ— æ•ˆæ‰£åˆ†
        if not (-90 <= row['latitude'] <= 90 and -180 <= row['longitude'] <= 180):
            score -= 10
        
        # åœ°éœ‡ç¼ºå°‘éœ‡çº§æ‰£åˆ†
        if row['type'] == 'EARTHQUAKE' and pd.isnull(row['magnitude']):
            score -= 10
        
        # æ•°æ®è¿‡æ—§æ‰£åˆ†ï¼ˆè¶…è¿‡7å¤©ï¼‰
        now = pd.Timestamp.now(tz='UTC')
        age_days = (now - row['timestamp']).total_seconds() / 86400
        if age_days > 7:
            score -= 5
        
        return max(0, score)
    
    # ä½¿ç”¨applyä¸ºæ¯è¡Œè®¡ç®—è´¨é‡åˆ†æ•°
    df_scored['data_quality_score'] = df_scored.apply(calculate_record_quality, axis=1)
    
    return df_scored
```

---

## ä¸‰ã€å®Œæ•´ETLæµç¨‹é›†æˆ

### 3.1 ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµæ°´çº¿

```python
class ETLProcessor:
    """å®Œæ•´çš„ETLå¤„ç†å™¨ï¼Œé›†æˆæ•°æ®æ¨¡å‹å’Œè´¨é‡ç›‘æ§"""
    
    def __init__(self):
        self.quality_monitor = None
        self.unified_df = None
    
    def process(
        self,
        usgs_data: Dict,
        nasa_data: Dict,
        gdacs_data: Dict
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„ETLæµç¨‹
        è¿”å›ç»Ÿä¸€DataFrameå’Œè´¨é‡æŠ¥å‘Š
        """
        # Step 1: Transform - è½¬æ¢ä¸ºç»Ÿä¸€æ¨¡å‹
        usgs_df = transform_usgs_to_unified(usgs_data)
        nasa_df = transform_nasa_to_unified(nasa_data)
        gdacs_df = transform_gdacs_to_unified(gdacs_data)
        
        # Step 2: Merge - åˆå¹¶æ•°æ®
        self.unified_df = merge_multi_source_data(usgs_df, nasa_df, gdacs_df)
        
        # Step 3: Quality Assessment - è´¨é‡è¯„ä¼°
        self.quality_monitor = DataQualityMonitor(self.unified_df)
        quality_report = self.quality_monitor.assess_data_quality()
        
        # Step 4: Auto Fix - è‡ªåŠ¨ä¿®å¤
        self.unified_df = auto_fix_quality_issues(self.unified_df)
        
        # Step 5: Add Quality Scores - æ·»åŠ è´¨é‡åˆ†æ•°
        self.unified_df = add_quality_score_to_dataframe(self.unified_df)
        
        return {
            'data': self.unified_df,
            'quality_report': quality_report,
            'record_count': len(self.unified_df),
            'source_breakdown': self.unified_df['source'].value_counts().to_dict()
        }
    
    def export_to_csv(self, filepath: str):
        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        self.unified_df.to_csv(filepath, index=False, encoding='utf-8')
    
    def export_to_json(self, filepath: str):
        """å¯¼å‡ºä¸ºJSONæ–‡ä»¶"""
        self.unified_df.to_json(filepath, orient='records', date_format='iso')
```

---

## å››ã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 4.1 Pandasé«˜æ€§èƒ½å®è·µ

```python
# ä¼˜åŒ–1ï¼šä½¿ç”¨categoryç±»å‹å‡å°‘å†…å­˜å ç”¨
df['type'] = df['type'].astype('category')
df['severity'] = df['severity'].astype('category')
df['source'] = df['source'].astype('category')

# ä¼˜åŒ–2ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
# âŒ æ…¢é€Ÿæ–¹æ³•ï¼ˆé€è¡Œå¤„ç†ï¼‰
for i, row in df.iterrows():
    df.at[i, 'age_days'] = (pd.Timestamp.now() - row['timestamp']).days

# âœ… å¿«é€Ÿæ–¹æ³•ï¼ˆå‘é‡åŒ–ï¼‰
df['age_days'] = (pd.Timestamp.now() - df['timestamp']).dt.days

# ä¼˜åŒ–3ï¼šä½¿ç”¨query()è¿›è¡Œé«˜æ•ˆè¿‡æ»¤
# âŒ æ…¢é€Ÿæ–¹æ³•
result = df[(df['type'] == 'EARTHQUAKE') & (df['magnitude'] >= 5.0)]

# âœ… å¿«é€Ÿæ–¹æ³•
result = df.query('type == "EARTHQUAKE" and magnitude >= 5.0')

# ä¼˜åŒ–4ï¼šä½¿ç”¨Pandasçš„å†…ç½®å‡½æ•°
# âŒ æ…¢é€Ÿæ–¹æ³•
df['latitude_rounded'] = df['latitude'].apply(lambda x: round(x, 2))

# âœ… å¿«é€Ÿæ–¹æ³•
df['latitude_rounded'] = df['latitude'].round(2)
```

### 4.2 å¤§æ•°æ®é‡å¤„ç†

```python
def process_large_dataset(data_chunks: List[Dict]) -> pd.DataFrame:
    """
    åˆ†å—å¤„ç†å¤§æ•°æ®é›†ï¼Œé¿å…å†…å­˜æº¢å‡º
    """
    dfs = []
    
    for chunk in data_chunks:
        # å¤„ç†æ¯ä¸ªchunk
        chunk_df = transform_usgs_to_unified(chunk)
        dfs.append(chunk_df)
    
    # ä½¿ç”¨concatåˆå¹¶æ‰€æœ‰chunk
    result = pd.concat(dfs, ignore_index=True)
    
    return result

# ä½¿ç”¨chunksizeè¯»å–å¤§å‹CSV
for chunk in pd.read_csv('large_file.csv', chunksize=10000):
    process_chunk(chunk)
```

---

## äº”ã€ç›‘æ§ä¸æ—¥å¿—

### 5.1 è´¨é‡ç›‘æ§å¯è§†åŒ–

```python
def generate_quality_dashboard(quality_report: Dict) -> str:
    """
    ç”Ÿæˆè´¨é‡ç›‘æ§ä»ªè¡¨æ¿çš„HTMLæŠ¥å‘Š
    """
    html = f"""
    <h2>æ•°æ®è´¨é‡ç›‘æ§æŠ¥å‘Š</h2>
    <div class="quality-summary">
        <h3>ç»¼åˆè´¨é‡åˆ†æ•°: {quality_report['overall_score']:.2f}/100</h3>
        <ul>
            <li>å®Œæ•´æ€§: {quality_report['completeness']['score']:.2f}/100</li>
            <li>å‡†ç¡®æ€§: {quality_report['accuracy']['score']:.2f}/100</li>
            <li>ä¸€è‡´æ€§: {quality_report['consistency']['score']:.2f}/100</li>
            <li>åŠæ—¶æ€§: {quality_report['timeliness']['score']:.2f}/100</li>
            <li>æœ‰æ•ˆæ€§: {quality_report['validity']['score']:.2f}/100</li>
        </ul>
    </div>
    """
    return html
```

### 5.2 æ—¥å¿—è®°å½•

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_with_logging(data: Dict):
    """å¸¦æ—¥å¿—çš„æ•°æ®å¤„ç†"""
    logger.info(f"Starting ETL process with {len(data)} records")
    
    try:
        df = transform_usgs_to_unified(data)
        logger.info(f"Successfully transformed {len(df)} records")
        
        quality_report = DataQualityMonitor(df).assess_data_quality()
        logger.info(f"Data quality score: {quality_report['overall_score']:.2f}")
        
        return df
    except Exception as e:
        logger.error(f"ETL process failed: {str(e)}")
        raise
```

---

## å…­ã€å®é™…åº”ç”¨ç¤ºä¾‹

### 6.1 å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
# ç¤ºä¾‹ï¼šå¤„ç†å®é™…æ•°æ®
if __name__ == "__main__":
    # 1. è·å–åŸå§‹æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    usgs_data = fetch_usgs_api()
    nasa_data = fetch_nasa_api()
    gdacs_data = fetch_gdacs_api()
    
    # 2. åˆ›å»ºETLå¤„ç†å™¨
    processor = ETLProcessor()
    
    # 3. æ‰§è¡ŒETLæµç¨‹
    result = processor.process(usgs_data, nasa_data, gdacs_data)
    
    # 4. æ‰“å°è´¨é‡æŠ¥å‘Š
    print(f"æ€»è®°å½•æ•°: {result['record_count']}")
    print(f"è´¨é‡åˆ†æ•°: {result['quality_report']['overall_score']:.2f}/100")
    print(f"\næ•°æ®æºåˆ†å¸ƒ:")
    for source, count in result['source_breakdown'].items():
        print(f"  {source}: {count} æ¡")
    
    # 5. å¯¼å‡ºæ•°æ®
    processor.export_to_csv('unified_hazards.csv')
    processor.export_to_json('unified_hazards.json')
    
    # 6. æŸ¥çœ‹æ•°æ®è´¨é‡è¯¦æƒ…
    quality_report = result['quality_report']
    print(f"\nå®Œæ•´æ€§: {quality_report['completeness']['completeness_rate']}%")
    print(f"å‡†ç¡®æ€§: {quality_report['accuracy']['coord_accuracy']}%")
    print(f"åŠæ—¶æ€§: {quality_report['timeliness']['recent_data_rate']}%")
```

---

## ä¸ƒã€é¡¹ç›®æˆæœ

### 7.1 é‡åŒ–æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **æ•°æ®å‡†ç¡®ç‡** | 99.8% | æ—¶é—´æˆ³è§£æå‡†ç¡®ç‡ |
| **åæ ‡æœ‰æ•ˆæ€§** | 100% | ç»çº¬åº¦èŒƒå›´éªŒè¯ |
| **æ•°æ®å®Œæ•´æ€§** | 98.5% | å¿…å¡«å­—æ®µå®Œæ•´ç‡ |
| **è´¨é‡åˆ†æ•°** | 98+ | ç»¼åˆè´¨é‡è¯„åˆ† |
| **å¤„ç†æ€§èƒ½** | <50ms | 1000æ¡æ•°æ®å¤„ç†æ—¶é—´ |
| **å†…å­˜ä¼˜åŒ–** | 70% | ä½¿ç”¨categoryç±»å‹å |

### 7.2 æŠ€æœ¯ä¼˜åŠ¿

- **Pandaså‘é‡åŒ–è¿ç®—**ï¼šæ¯”å¾ªç¯å¿«10-100å€
- **categoryç±»å‹**ï¼šå†…å­˜å ç”¨å‡å°‘70%
- **è‡ªåŠ¨åŒ–è´¨é‡ç›‘æ§**ï¼šé›¶äººå·¥å¹²é¢„
- **å¯æ‰©å±•æ¶æ„**ï¼šè½»æ¾æ·»åŠ æ–°æ•°æ®æº
- **å®Œæ•´çš„è´¨é‡ä½“ç³»**ï¼š5ç»´åº¦å…¨é¢è¯„ä¼°

---

## å…«ã€æ€»ç»“

æœ¬æ–‡æ¡£å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨**Pandas**æ„å»ºç”Ÿäº§çº§çš„ç»Ÿä¸€æ•°æ®æ¨¡å‹å’Œè´¨é‡ç›‘æ§ä½“ç³»ï¼š

1. **ç»Ÿä¸€æ•°æ®æ¨¡å‹**ï¼šå®šä¹‰æ ‡å‡†Schemaï¼Œå®ç°å¤šæºå¼‚æ„æ•°æ®çš„ç»Ÿä¸€è½¬æ¢
2. **äº”ç»´è´¨é‡ç›‘æ§**ï¼šå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€åŠæ—¶æ€§ã€æœ‰æ•ˆæ€§å…¨é¢è¯„ä¼°
3. **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šETLæµç¨‹è‡ªåŠ¨åŒ–ï¼Œè´¨é‡é—®é¢˜è‡ªåŠ¨ä¿®å¤
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå‘é‡åŒ–è¿ç®—ã€categoryç±»å‹ã€é«˜æ•ˆå†…å­˜ç®¡ç†
5. **å¯è§†åŒ–ç›‘æ§**ï¼šè´¨é‡ä»ªè¡¨æ¿ã€æ—¥å¿—è®°å½•ã€è¯¦ç»†æŠ¥å‘Š

è¿™å¥—ä½“ç³»ç¡®ä¿äº†**æ—¥å¤„ç†1000+æ¡æ•°æ®**ï¼Œ**æ•°æ®è´¨é‡åˆ†æ•°98%+**ï¼Œä¸ºä¸šåŠ¡å†³ç­–æä¾›å¯é çš„æ•°æ®åŸºç¡€ã€‚

---

**é¡¹ç›®é“¾æ¥**: [github.com/Jamie-qian/prometheus-global-guardian](https://github.com/Jamie-qian/prometheus-global-guardian)  
**ä½œè€…**: Jamie0807  
**æœ€åæ›´æ–°**: 2025å¹´12æœˆ2æ—¥
